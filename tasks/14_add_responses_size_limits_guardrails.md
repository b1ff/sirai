a couple of times due to mistake or specific request of llm, the context send to input was too big, which results into high cost of usage.
